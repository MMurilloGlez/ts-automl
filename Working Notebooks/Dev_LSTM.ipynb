{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amateur-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from extract_features import *      \n",
    "from extract_sample_features import *    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "respective-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, targetcol = 'VALUE', datecol = 'DATE', sep = ';', \n",
    "              decimal = ',', date_format = \"%d/%m/%Y %H:%M:%S.%f\", freq = '1T'):\n",
    "    \n",
    "    if filename[-3:] == 'csv' or filename[-3:] == 'txt':\n",
    "        file_ = pd.read_table(filename, sep = sep, decimal = decimal, parse_dates = False)\n",
    "        file_parsed = parsedates(file_, date_format = date_format, freq = freq, datecol = datecol)\n",
    "        \n",
    "    elif filename[-3:] == 'xls' or filename[-3:] == 'lsx':\n",
    "        file_ = pd.read_excel(filename, parse_dates = False)\n",
    "        file_parsed = parsedates(file_, date_format = date_format, freq = freq,datecol = datecol)\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"This file type is not supported\" )\n",
    "    \n",
    "    return(file_parsed[targetcol])\n",
    "\n",
    "def parsedates(file_, date_format, freq, datecol):\n",
    "    freq = pd.to_timedelta(freq)\n",
    "    datetime_i = pd.to_datetime(file_[datecol], format = date_format)\n",
    "    print(type(datetime_i.iloc[-1]))\n",
    "    if freq < (datetime_i.iloc[1] - datetime_i.iloc[0]):\n",
    "        raise ValueError('Expected frequency is smaller than dataset information')\n",
    "        \n",
    "    file_.index = datetime_i\n",
    "    desired_index = pd.date_range(start = datetime_i.iloc[0], end = datetime_i.iloc[-1], freq = freq)\n",
    "    \n",
    "    file_int = file_.reindex(file_.index.union(desired_index)).interpolate(method = 'time').reindex(desired_index)\n",
    "    \n",
    "    return(file_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "requested-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_split(data, test_size=50):\n",
    "    ts_train, ts_test = train_test_split(data, shuffle=False)\n",
    "    print('Test size: ', len(ts_test), '  Input size: ', len(data))\n",
    "    return(ts_train, ts_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expired-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_creation(X, lags=3, step=1, dropna = False):\n",
    "    X = X.iloc[:,0]\n",
    "    print(type(X))\n",
    "    if type(X) is pd.DataFrame:\n",
    "        new_dict = {}\n",
    "        for col_name in X:                                                          \n",
    "            for l in range(1,self.lag+1, self.step):                                \n",
    "                new_dict['%s_lag%d' %(col_name,l)]=X[col_name].shift(l)            \n",
    "        res=pd.DataFrame(new_dict,index=X.index)  \n",
    "\n",
    "    elif type(X) is pd.Series:                                                      ## si es serie\n",
    "        the_range=range(0,lags+1, step)                                    ## de 0 a numero de lags\n",
    "        res=pd.concat([X.shift(i) for i in the_range],axis=1)                       ## concatenamos los lags a cada fila del nuevo dataframe \n",
    "        res.columns=['lag_%d' %i for i in the_range]\n",
    "    if dropna:                                                                 ## si hay que quitar na\n",
    "        res = res.dropna()                                                          ## quitamos na del principio del df\n",
    "        res = res[res.columns[::-1]]                                                ## res = todo menos la ultima columna \n",
    "        return res\n",
    "    else:\n",
    "        res = res[res.columns[::-1]]                                                ## res = todo menos la ultima columna\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-shareware",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ancient-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 100                        ## tamaño del test para entrenar el regresor\n",
    "window_length = 20                  ## tamaño de ventana para la prediccion de ventana\n",
    "rolling_window = [5,10,20]          ## Cuantas caracteristicas debe tener la rolling window para el calculo de features rollo moving average\n",
    "horizon = 1                         ## horizonte a predecir en cada recursion\n",
    "step = 1                            ## paso que se mueve la ventana para cada recursión.\n",
    "freq = '5s'                         ## Frecuencia de la serie de entrada o de prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "square-gender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2018-11-13 22:30:00    276.036780\n",
       "2018-11-13 22:50:00    275.003310\n",
       "2018-11-13 23:10:00    261.639657\n",
       "2018-11-13 23:30:00    245.255448\n",
       "2018-11-13 23:50:00    242.053699\n",
       "Freq: 20T, Name: INSTALACIONES [kWh], dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = '20T'\n",
    "filename = 'Seat.csv'\n",
    "\n",
    "df = read_data(filename='Seat.csv',\n",
    "                 freq='20T',\n",
    "                 targetcol='INSTALACIONES [kWh]',\n",
    "                 datecol='MSJO_DATUM',\n",
    "                 sep=',',\n",
    "                 decimal='.',\n",
    "                 date_format=\"%d/%m/%Y %H:%M\")\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sacred-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_features(X, window_length = 5, features = [], rolling_window = [2]):      ## Creacion de features para los datos de entrada (se llama solo una vez)\n",
    "    lags_X = lags_sample(X, window_length=window_length, step=1)                            ## Creacion de los lags (min t-1, -2 ....)\n",
    "    df = lags_X.copy()                                                                      ## Copia a otro dataframe for some reason\n",
    "    for f in features:                                                                      ## Features contiene la lista de features que queremos añadir al dataframe con lags.b\n",
    "        if rolling_window:                                                                  ## si se ha especificado una rolling window\n",
    "            for rol in rolling_window:   \n",
    "                aux = switch_sample_features(f, X, lags_X, rol)                             ## Variable auxiliar para crear la feature (se borra a cada iteracion) con los lags (la fila vaya)\n",
    "                df = pd.concat([df, aux], axis = 1).dropna()                                ## concatenamos el valor al dataframe\n",
    "        else:\n",
    "            aux = switch_sample_features(f, X, lags_X, [])                                  ## opcion sin rolling windows\n",
    "            df = pd.concat([df, aux], axis = 1).dropna()                                    ## añadir a dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "recent-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_sample_features(value, X, lags_X, rolling_window):                               ## Switch con el que elegir las caracteristicas del sample.\n",
    "    return { \n",
    "        'mean': lambda : mean_sample(lags_X, rolling_window),\n",
    "        'std': lambda : std_sample(lags_X, rolling_window),\n",
    "        'max': lambda : max_sample(lags_X, rolling_window),\n",
    "        'min': lambda : min_sample(lags_X, rolling_window),\n",
    "        'quantile': lambda : quantile_sample(lags_X, rolling_window),\n",
    "        'iqr': lambda : iqr_sample(lags_X, rolling_window),\n",
    "        'entropy': lambda : entropy_sample(lags_X, rolling_window),\n",
    "        'trimmean': lambda : trimmean_sample(lags_X, rolling_window),\n",
    "        'variation': lambda : variation_sample(lags_X, rolling_window),\n",
    "        'hmean': lambda : hmean_sample(lags_X, rolling_window),\n",
    "        'gmean': lambda : gmean_sample(lags_X, rolling_window),\n",
    "        'mad': lambda : mad_sample(lags_X, rolling_window),\n",
    "#         'gstd': lambda : gstd_sample(lags_X, rolling_window),\n",
    "        'tvar': lambda : tvar_sample(lags_X, rolling_window),\n",
    "        'kurtosis': lambda : kurtosis_sample(lags_X, rolling_window),\n",
    "        'sem': lambda : sem_sample(lags_X, rolling_window),\n",
    "        'wav': lambda : wav_sample(lags_X, rolling_window),\n",
    "        #-----------------------------------------\n",
    "        'minute': lambda : minute_sample(X),\n",
    "        'hour': lambda : hour_sample(X),\n",
    "        'dayofweek': lambda : dayofweek_sample(X),\n",
    "        'day': lambda : day_sample(X),\n",
    "        'month': lambda : month_sample(X),\n",
    "        'quarter': lambda : quarter_sample(X),\n",
    "        'weekofyear': lambda : weekofyear_sample(X),\n",
    "        'weekend': lambda : weekend_sample(X)\n",
    "        \n",
    "    }.get(value)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pointed-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_horizon(df, horizon):\n",
    "    y = df.copy()\n",
    "    ht = HorizonTransformer(horizon = horizon+1)\n",
    "    y_horizon = ht.fit_transform(y.iloc[:,0]) #Los horizon ultimos valores son nan ya que no se puede crear un horizonte completo para ellos\n",
    "    y_horizon = pd.DataFrame(y_horizon[:-horizon, :], index = y.index[horizon:])\n",
    "    name = 't+'+str(horizon)\n",
    "    y_horizon = pd.DataFrame({name : y_horizon.iloc[:, -1]})\n",
    "    \n",
    "    return y_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "requested-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y, num_features):                                ## Elegimos componente principales entrenando un lgbmr y extrayendo los best_features\n",
    "    clf = lgb.LGBMRegressor(n_estimators=40).fit(X, y)                    ## Fit modelo con poquitos estimators (40)\n",
    "    best_indx_col = clf.feature_importances_.argsort()[-num_features:]    ## elegimos el num_features que nos queramos quedar\n",
    "    best_features = list(X_train.columns[best_indx_col])                  ## Creamos lista para indexar el dataframe\n",
    "    return order_by_other_list(X_train.columns, best_features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "organized-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_by_other_list(list_order, list_tobe_order):                     ## Orden de la lista y lista a ordenar\n",
    "    d = {k:v for v,k in enumerate(list_order)}                            ## creas un dict key:value for enumerate tal\n",
    "    list_tobe_order.sort(key=d.get)                                       ## le das un sort con key = d.get (pillamos la key del dict)\n",
    "    return list_tobe_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parallel-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_features(value, lags_data, date, rolling_window, num):         ## Switch igual que antes, se añade el parametro num, que significa?\n",
    "    return {\n",
    "        'lag': lambda : lag_feature(lags_data, num),\n",
    "        #--------------------------------------------------------------\n",
    "        'mean': lambda : mean_feature(lags_data, rolling_window, num),\n",
    "        'std': lambda : std_feature(lags_data, rolling_window, num),\n",
    "        'max': lambda : max_feature(lags_data, rolling_window, num),\n",
    "        'min': lambda : min_feature(lags_data, rolling_window, num),\n",
    "        'quantile': lambda : quantile_feature(lags_data, rolling_window, num),\n",
    "        'iqr': lambda : iqr_feature(lags_data, rolling_window, num),\n",
    "        'entropy': lambda : entropy_feature(lags_data, rolling_window, num),\n",
    "        'trimmean': lambda : trimmean_feature(lags_data, rolling_window, num),\n",
    "        'variation': lambda : variation_feature(lags_data, rolling_window, num),\n",
    "        'hmean': lambda : hmean_feature(lags_data, rolling_window, num),\n",
    "        'gmean': lambda : gmean_feature(lags_data, rolling_window, num),\n",
    "        'mad': lambda : mad_feature(lags_data, rolling_window, num),\n",
    "#         'gstd': lambda : gstd_feature(lags_data, rolling_window, num),\n",
    "        'tvar': lambda : tvar_feature(lags_data, rolling_window, num),\n",
    "        'kurtosis': lambda : kurtosis_feature(lags_data, rolling_window, num),\n",
    "        'sem': lambda : sem_feature(lags_data, rolling_window, num),\n",
    "        'wav': lambda : wav_feature(lags_data, rolling_window, num),\n",
    "        #--------------------------------------------------------------\n",
    "        'minute': lambda : minute_feature(date, num),\n",
    "        'hour': lambda : hour_feature(date, num),\n",
    "        'dayofweek': lambda : dayofweek_feature(date, num),\n",
    "        'day': lambda : day_feature(date, num),\n",
    "        'month': lambda : month_feature(date, num),\n",
    "        'quarter': lambda : quarter_feature(date, num),\n",
    "        'weekofyear': lambda : weekofyear_feature(date, num),\n",
    "        'weekend': lambda : weekend_feature(date, num)\n",
    "    }.get(value)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "static-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(feature_names, lags_data, date):                       ## Creacion de features para las ventanas segun avanzan (se llama varias veces)\n",
    "    features = []                                                          ## Lista vacía de features para inicializar\n",
    "    for f in feature_names:                                                ## Por cada feature que nos pidan añadir\n",
    "        ftype, info = f.split('_',1)                                       ## Separamos el str a la altura de la barrabaja para separar en tipo de feature (p. ej minute) y numero (p ej 60)\n",
    "        if '_' in info:                                                    ## Si nos queda otra barrabaja es que es dato calculado con una rolling window\n",
    "            rolling_window, num = info.split('_')                          ## separamos en rolling window y numero\n",
    "            num = int(num)                                                 ## str a int\n",
    "            rolling_window = int(rolling_window)                           ## str a int\n",
    "        else:\n",
    "            num = int(info)                                                ## tambien puede ser que no se de el caso de incluir el ftype en el nombre, asi que seguimos adelante simplemente sin rolling window\n",
    "            rolling_window = None\n",
    "        features.append(switch_features(ftype, lags_data, date, rolling_window, num))\n",
    "                                \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "developed-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_optimization(value, regressor):                                  ## Switch para elegir optimizaciones del regresor\n",
    "    return {\n",
    "        'tpe': lambda : tpe_optimization(),                                 ## TPE Tree-structured Parzen optimization (Secuencial)\n",
    "        'pso': lambda : pso_optimization(),                                 ## PSO Particle Swarm Optimization (Enjambre)\n",
    "    }.get(value)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continued-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['mean', 'std', 'max', 'min', 'quantile', 'iqr', 'entropy', 'trimmean', 'variation', 'hmean', 'gmean', 'mad', 'tvar',\n",
    "            'kurtosis', 'sem', 'minute', 'hour', 'dayofweek', 'day', 'month', 'quarter', 'weekofyear', 'weekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "compact-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    " def recursive_forecast(y, model, window_length, feature_names, rolling_window, n_steps): ## modelo debe estar pre-entrenado\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: pd.Series holding the input time-series to forecast\n",
    "    model: pre-trained machine learning model\n",
    "    lags: list of lags used for training the model\n",
    "    n_steps: number of time periods in the forecasting horizon\n",
    "    step: forecasting time period\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    fcast_values: pd.Series with forecasted values\n",
    "    \"\"\"\n",
    "   \n",
    "    # get the dates to forecast\n",
    "    last_date = y.index[-1] + datetime.timedelta(minutes=15)                     ## ultima fecha, le añadimos un delay de 15 mins porque?¿ usamos 15 minutos de validacion o test \n",
    "    target_range = pd.date_range(last_date, periods=n_steps, freq=freq)          ## creamos el target a partir del ultimo dia. \n",
    "    target_value = np.arange(n_steps, dtype = float)                             ## Si no guardamos como dtype float nos guarda como enteros\n",
    "    max_rol = max(rolling_window, default=1)                                     ## Maximo del array del rolling window\n",
    "    lags = list(y.iloc[-(window_length+(max_rol-1)):,0].values)                  ## creacion de lags en forma de lista. Indexando el valor de las ys (solo hay ys porque es una serie temporal duh)\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    for i in range(n_steps):                                                     ## Para cada uno de los steps\n",
    "        train = create_features(feature_names, lags, target_range[i])            ## \n",
    "        new_value = model.predict(pd.DataFrame(train).transpose())               ## Calculo del nuevo valor\n",
    "        target_value[i] = new_value[0]                                           ## Guardamos el valor en el target\n",
    "        lags.pop(0)                                                              ## Quitamos el primer valor del lag\n",
    "        lags.append(new_value[0])                                                ## añadimos la prediccion al final del lag\n",
    "                                                                                 ## Y volvemos a gestionar con el nuevo lag\n",
    "           \n",
    "    return target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "incoming-peace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53131 100\n"
     ]
    }
   ],
   "source": [
    "y_train, y_test = train_test_split(df, test_size=points, shuffle=False)      ## Split en train y test sin shuffle por ser time series. Test_size se da como points\n",
    "print(y_train.shape[0], y_test.shape[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "respected-poster",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Too many indexers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexingError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f59e44aeec4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sample_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrolling_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrolling_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2813142ceefc>\u001b[0m in \u001b[0;36mcreate_sample_features\u001b[0;34m(X, window_length, features, rolling_window)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_sample_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrolling_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0;31m## Creacion de features para los datos de entrada (se llama solo una vez)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlags_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlags_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m                            \u001b[0;31m## Creacion de los lags (min t-1, -2 ....)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlags_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                                      \u001b[0;31m## Copia a otro dataframe for some reason\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m                                                                      \u001b[0;31m## Features contiene la lista de features que queremos añadir al dataframe con lags.b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrolling_window\u001b[0m\u001b[0;34m:\u001b[0m                                                                  \u001b[0;31m## si se ha especificado una rolling window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/Recursive_Forecast/extract_sample_features.py\u001b[0m in \u001b[0;36mlags_sample\u001b[0;34m(df, window_length, step)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlags_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                       \u001b[0;31m## lags de un sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m                              \u001b[0;31m## lags dentro de lags para crear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m                                                                  \u001b[0;31m## nos quedamos todo menos la ultima columna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/Recursive_Forecast/extract_sample_features.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                                     \u001b[0;31m## Transformacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                                                                 \u001b[0;31m## Convertimos a un df unidimensional (or series for that matter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m                                                     \u001b[0;31m## si es dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnew_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m                                                                 \u001b[0;31m## Creamos un dict vacio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    887\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1450\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1451\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mCheck\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0macross\u001b[0m \u001b[0mmy\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \"\"\"\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key_length\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexingError\u001b[0m: Too many indexers"
     ]
    }
   ],
   "source": [
    "X_train = create_sample_features(y_train, window_length=window_length, features=features, rolling_window=rolling_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat = 40\n",
    "%time best_features = feature_selection(X_train, y_horizon.values.ravel(), selected_feat)\n",
    "\n",
    "X_train_selec = X_train.loc[:, best_features]\n",
    "X_train_selec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-blackberry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
